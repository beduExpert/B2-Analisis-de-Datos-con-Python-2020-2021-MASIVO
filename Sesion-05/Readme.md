
# Sesi√≥n 05: Distribuciones muestrales y t√©cnicas de evaluaci√≥n de modelos

## :dart: Objetivos

- Distinguir la diferencia entre poblaci√≥n y muestra.
- Entender el concepto de 'sesgos' y por qu√© es tan importante estar conscientes de ellos.
- Aprender el concepto de muestreo aleatorio y c√≥mo puede protegernos parcialmente de los sesgos.
- Utilizar la t√©cnica 'bootstrap' como medio para explorar la distribuci√≥n muestral de una estad√≠stica.
- Crear y utilizar histogramas, errores est√°ndar e intervalos de confianza para evaluar la incertidumbre de una medida estad√≠stica.
- Utilizar t√©cnicas para evitar sesgos en el entrenamiento de modelos, como la divisi√≥n de datasets y la validaci√≥n cruzada.

## üìÇ Contenido

<ins>Poblaciones y muestras</ins>

En un estudio estad√≠stico la *poblaci√≥n* es el grupo completo acerca del cual se pretende obtener cierta informaci√≥n. Una *muestra* es un subconjunto de esa poblaci√≥n.

Es muy importante antes de empezar un an√°lisis estad√≠stico tener muy claro cu√°les son los par√°metros que definen a nuestra poblaci√≥n. Esos par√°metros pueden ser geogr√°ficos, √©tnicos, de g√©nero, profesionales, etc.

Una de las tareas que realizamos como cient√≠ficos de datos es la de definir poblaciones, obtener muestras de datos de estas poblaciones, y encontrar procedimientos a trav√©s de los que podamos encontrar informaci√≥n √∫til que pueda ser aplicable a una gran parte de la poblaci√≥n. Dado que muchas veces tener datos concisos de **toda** la poblaci√≥n es muy dif√≠cil, costoso o hasta imposible, debemos aprender a trabajar con muestras de manera efectiva.

Platiquemos un poco acerca de:

1. ¬øC√≥mo definir poblaciones?
2. ¬øA qu√© retos nos enfrentamos cuando definimos poblaciones?
3. ¬øCu√°les pueden ser las consecuencias de definir incorrectamente a una poblaci√≥n?
4. ¬øC√≥mo generalizamos informaci√≥n de una muestra a una poblaci√≥n?
5. ¬øQu√© problemas pueden surgir en este proceso de generalizaci√≥n?
6. ¬øC√≥mo podemos evitar algunos de estos problemas o aunque sea minimizarlos?

---

<ins>Sesgos en nuestros datos</ins>

Una de las dificultades a las que nos enfrentamos cuando trabajamos con muestras de datos es que hay una alta probabilidad de que nuestros datos est√©n sesgados. Esto quiere decir que tienen una tendencia que no refleja la realidad tal y como es.

Los sesgos son muy peligrosos. Dificultan y hasta imposibilitan la obtenci√≥n de informaci√≥n √∫til. Cuando nuestros datos est√°n sesgados, la informaci√≥n que obtengamos no puede ser generalizada  al resto de la poblaci√≥n, puesto que no refleja la realidad como es, s√≥lo una perspectiva de ella.

Entrenar modelos predictivos y clasificatorios utilizando datos sesgados puede resultar en enormes e inaceptables injusticias.

Como cient√≠ficos de datos debemos de pensar y estar atentos a los sesgos de nuestros datos, no solamente porque distorsionan nuestros an√°lisis, sino porque tenemos la responsabilidad de llevar a cabo nuestra profesi√≥n √©ticamente.

Platiquemos un poco acerca de:

1. ¬øDe d√≥nde surgen los sesgos en nuestros datos? ¬øC√≥mo llegan ah√≠?
2. ¬øA qu√© problemas podemos enfrentarnos cuando hay sesgos en nuestros datos?
3. ¬øQu√© podemos hacer para evitar sesgos en nuestros datos?
4. ¬øDe qu√© manera da√±an a la sociedad estos sesgos?
5. ¬øQu√© papel juegan los cient√≠ficos de datos en la eliminaci√≥n de estos sesgos?

---

<ins>Distribuciones muestrales de estad√≠sticas y Bootstrap</ins>

Para este momento seguramente ya te habr√°s percatado de que las medidas estad√≠sticas que hemos aprendido a hasta ahora no son infalibles. Cada vez que tomamos una medida estad√≠stica, existe cierta incertidumbre alrededor de ella. ¬øQu√© hubiera pasado si nuestra muestra fuera ligeramente distinta? ¬øNuestra medida se mantendr√≠a igual o cambiar√≠a?

Dado que normalmente no es posible regresar al origen de los datos para adquirir m√°s muestras, existen ciertos procedimientos que podemos realizar para determinar el nivel de incertidumbre que existe en nuestras medidas estad√≠sticas.

Una de estas t√©cnicas es el bootstrapping. Veamos c√≥mo funciona.

>

[**`Ejemplo 1`**](Ejemplo-01/bootstrap.ipynb)
[**`Reto 1`**](Reto-01/bootstrap.ipynb)

---

<ins>Error est√°ndar e intervalos de confianza</ins>

Ya que hemos tenemos una Serie con las medidas estad√≠sticas tomadas de cada remuestra, adem√°s de revisar la distribuci√≥n podemos hacer un par de cosas m√°s.

El error est√°ndar nos dice qu√© tan dispersas est√°n nuestras medidas estad√≠sticas. Esta es una de las maneras de cuantificar incertidumbre.

Los intervalos de confianza son una manera de mostrar la incertidumbre de una manera muy f√°cil de comprender, suelen utilizarse en ciencia de datos para establecer qu√© tan precisa es nuestra medida estad√≠stica.

>

[**`Ejemplo 2`**](Ejemplo-02/error_estandar_e_intervalos_de_confianza.ipynb)
[**`Reto 2`**](Reto-02/error_estandar_e_intervalos_de_confianza.ipynb)

---

<ins>T√©cnicas de evaluaci√≥n de modelos</ins>

As√≠ como podemos tener sesgos e incertidumbre en nuestras medidas estad√≠sticas, tambi√©n nos puede pasar lo mismo con los modelos predictivos que entrenamos.

Hay varias formas en las que podemos encontrar este tipo de problemas al entrenar modelos predictivos o clasificatorios. Aqu√≠ vamos a hablar de dos de ellas:

1. A veces puede suceder que los datos que tenemos ya tienen de por s√≠ un sesgo incluido. Este sesgo puede provenir de la forma en la que fueron recabados los datos, de los sesgos de las personas que los recabaron, de un error de metodolog√≠a, etc. Esto puede ocasionar que nuestros modelos entrenados no puedan realizar predicciones racionales. Platiquemos un poco acerca de esto:

  a) ¬øDe d√≥nde pueden provenir estos sesgos? ¬øC√≥mo llegan a nuestros datos?
  
  b) ¬øQu√© problemas pueden ocasionar? ¬øQu√© ejemplos tenemos de esto?
  
  c) ¬øC√≥mo podemos protegernos de este tipo de errores? ¬øEs posible eliminar por completo los sesgos en nuestros datos?

2. Existe la posibilidad de que un modelo que entrenemos resulte muy bueno para predecir los datos con los que fue entrenado, pero que no pueda generalizar su capacidad predictiva a datos que no ha visto anteriormente. Como normalmente tampoco es una opci√≥n regresar a la fuente de los datos para colectar nuestras muestras, necesitamos entonces algunas t√©cnicas que nos permitan tener un poco m√°s de confianza en nuestros modelos. A continuaci√≥n vamos a ver en la pr√°ctica dos maneras de evitar este problema.

---

<ins>Dataset de entrenamiento y dataset de prueba</ins>

Un primer acercamiento puede ser tomar nuestro dataset y dividirlo en dos: un dataset para entrenamiento y otro para pruebas. El modelo se entrena utilizando el dataset de entrenamiento y luego su precisi√≥n se eval√∫a utilizando el dataset de prueba. De esta manera se 'simula' la capacidad predictiva del modelo en el mundo real: probando su precisi√≥n con datos que nunca antes ha visto.

>

[**`Ejemplo 3`**](Ejemplo-03/entrenamiento_y_prueba.ipynb)
[**`Reto 3`**](Reto-03/entrenamiento_y_prueba.ipynb)

---

<ins>Validaci√≥n cruzada</ins>

La validaci√≥n cruzada lleva el m√©todo anterior a√∫n m√°s lejos, puesto que realiza m√∫ltiples divisiones, entrena el modelo m√∫ltiples veces usando combinaciones distintas de divisiones y eval√∫a al modelo usando el promedio de todos los entrenamientos.

>

[**`Ejemplo 4`**](Ejemplo-04/validacion_cruzada.ipynb)
[**`Reto 4`**](Reto-04/validacion_cruzada.ipynb)

---

## Postwork

[**`Postwork Sesi√≥n 5`**](Postwork/Readme.md)
